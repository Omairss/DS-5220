{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A: The value of S1 would be 2 since there are 2 feature and S3 would be 1 since the output is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_w_reg(A3, Y, parameters, lambd):\n",
    "  \n",
    "    m  = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    cross_entropy_cost     = np.squeeze(- 1 * np.sum(np.multiply(Y, np.log(A3)) \\\n",
    "                                                     + np.multiply((1 - Y), np.log(1 - A3)), axis=1) / Y.shape[1])\n",
    "    \n",
    "    L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2))) / (2 * m) \n",
    "    \n",
    "    \n",
    "    return cross_entropy_cost + L2_regularization_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(layer_dims):\n",
    "\n",
    "    parameters = {}\n",
    "    \n",
    "    for l in range(1, len(layer_dims)):\n",
    "        \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "          \n",
    "    return parameters\n",
    "\n",
    "def forward_prop_activation(A_prev, W, b, activation):\n",
    "    \n",
    "    Z            = np.dot(W, A) + b\n",
    "    linear_cache = (A, W, b)\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        \n",
    "        A                = np.maximum(Z, 0)\n",
    "        activation_cache = Z\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        \n",
    "        A                = (1 + np.exp(-Z))**(-1)\n",
    "        activation_cache = Z\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        \n",
    "        activation_cache = Z\n",
    "        e_z              = np.exp(Z)\n",
    "        e_nz             = np.exp(-Z)\n",
    "        A                = (e_z - e_nz)/(e_z + e_nz)\n",
    "\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def forward_prop(X, parameters, activations):\n",
    "\n",
    "    caches = []\n",
    "    A      = X\n",
    "    L      = len(parameters) // 2 \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        \n",
    "        A_prev     = A\n",
    "        activation = activations.popleft()\n",
    "        A, cache   = forward_prop_activation(A_prev, parameters[\"W\" + str(l)],parameters[\"b\" + str(l)], activation)\n",
    "        caches.append(cache)\n",
    "\n",
    "    activation = activations.popleft()\n",
    "    AL, cache  = forward_prop_activation(A, parameters[\"W\" + str(l + 1)], parameters[\"b\" + str(l + 1)], activation)\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(dZ, cache, lambd):\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m            = A_prev.shape[1]\n",
    "    \n",
    "    dW           = (1/m) * np.dot(dZ, A_prev.T) + (lambd * W) / m\n",
    "    db           = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev      = np.dot(W.T, dZ)\n",
    "    \n",
    "\n",
    "    return (dA_prev, dW, db)\n",
    "\n",
    "\n",
    "def backprop_activation(dA, cache, activation, lambd):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        Z          = activation_cache\n",
    "        dZ         = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "    \n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        \n",
    "        Z  = activation_cache\n",
    "        s  = np.power((1 + np.exp(-Z)),-1)\n",
    "        dZ = dA * s * (1 - s)\n",
    "   \n",
    "        \n",
    "    elif activation == \"tanh\":        \n",
    "        \n",
    "        Z    = activation_cache\n",
    "        s    = (np.exp(Z) - np.exp(-Z))/(np.exp(Z) + np.exp(-Z))    \n",
    "        A    = Z * (1 - np.power(s, 2))\n",
    "    \n",
    "    dA_prev, dW, db = backprop(dZ, linear_cache, lambd)\n",
    "    \n",
    "    return (dA_prev, dW, db)\n",
    "\n",
    "def get_backprop_gradients(AL, Y, caches, activations, lambd):\n",
    "\n",
    "    grads = {}\n",
    "    L     = len(caches)\n",
    "    m     = AL.shape[1]\n",
    "    Y     = Y.reshape(AL.shape)\n",
    "        \n",
    "    dAL   = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    \n",
    "    activation    = activations.pop()\n",
    "    current_cache = caches[L-1]\n",
    "    \n",
    "    grads[\"dA\" + str(L-1)],\\\n",
    "    grads[\"dW\" + str(L)],\\\n",
    "    grads[\"db\" + str(L)] = backprop_activation(dAL, current_cache, activation, lambd)\n",
    "        \n",
    "    for l in reversed(range(L-1)):\n",
    "        \n",
    "        activation    = activations.pop()\n",
    "        current_cache = caches[l]\n",
    "        \n",
    "        dA_prev_temp, dW_temp, db_temp = backprop_activation(grads[\"dA\" + str(l + 1)],\\\n",
    "                                                                    current_cache, activation, lambd)\n",
    "        grads[\"dA\" + str(l)]     = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "    return grads\n",
    "    \n",
    "    \n",
    "def backward_w_reg(X, Y, cache, lambd):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2) = cache\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1. / m * np.dot(dZ2, A1.T) + (lambd * W2) / m\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    \n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1. / m * np.dot(dZ1, X.T) + (lambd * W1) / m\n",
    "    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dA2\": dA2,\"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2 \n",
    "    \n",
    "    for l in range(L):\n",
    "        \n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - \\\n",
    "                                        learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n",
    "                                        learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def train_model(X, Y, config):\n",
    "            \n",
    "    parameters = init(config[\"layers_dims\"])\n",
    "    \n",
    "    for i in range(0, config[\"num_iterations\"]):\n",
    "        \n",
    "        fact       = deque(config[\"activations\"].copy())\n",
    "        AL, caches = forward_prop(X, parameters, fact)          \n",
    "        cost       = get_cost_w_reg(AL, Y, parameters, config[\"lambd\"])\n",
    "        bact       = deque(config[\"activations\"].copy())\n",
    "        grads      = get_backprop_gradients(AL, Y, caches, bact, config[\"lambd\"])\n",
    "        parameters = update(parameters, grads, config[\"learning_rate\"])\n",
    "        \n",
    "        if i % 1000 == 0:  print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "                         \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def predict(X, y, parameters, activations):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((1,m))\n",
    "\n",
    "\n",
    "    facts          = deque(activations.copy())\n",
    "    probas, caches = forward_prop(X, parameters, facts)\n",
    "\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def load():\n",
    "    \n",
    "    \n",
    "    input_data    = pd.read_csv('data/HW5.csv')\n",
    "    train, test   = train_test_split(input_data, test_size = 0.2)\n",
    "    \n",
    "    x_train       = np.array(train.loc[:, ['a', 'b']])\n",
    "    y_train       = np.array(train.loc[:, ['label']])\n",
    "    \n",
    "    x_test        = np.array(test.loc[:, ['a', 'b']])\n",
    "    y_test        = np.array(test.loc[:, ['label']])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. The accuracy for S = 2 is 73%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693154\n",
      "Cost after iteration 50: 0.693137\n",
      "Cost after iteration 100: 0.693122\n",
      "Cost after iteration 150: 0.693104\n",
      "Cost after iteration 200: 0.693073\n",
      "Cost after iteration 250: 0.693006\n",
      "Cost after iteration 300: 0.692856\n",
      "Cost after iteration 350: 0.692502\n",
      "Cost after iteration 400: 0.691650\n",
      "Cost after iteration 450: 0.689604\n",
      "Cost after iteration 500: 0.684798\n",
      "Cost after iteration 550: 0.674145\n",
      "Cost after iteration 600: 0.653417\n",
      "Cost after iteration 650: 0.621882\n",
      "Cost after iteration 700: 0.587810\n",
      "Cost after iteration 750: 0.560301\n",
      "Cost after iteration 800: 0.540220\n",
      "Cost after iteration 850: 0.524147\n",
      "Cost after iteration 900: 0.508574\n",
      "Cost after iteration 950: 0.491013\n",
      "Cost after iteration 1000: 0.471099\n",
      "Cost after iteration 1050: 0.451062\n",
      "Cost after iteration 1100: 0.433623\n",
      "Cost after iteration 1150: 0.419801\n",
      "Cost after iteration 1200: 0.409139\n",
      "Cost after iteration 1250: 0.400994\n",
      "Cost after iteration 1300: 0.394644\n",
      "Cost after iteration 1350: 0.389544\n",
      "Cost after iteration 1400: 0.385338\n",
      "Cost after iteration 1450: 0.381857\n",
      "Cost after iteration 1500: 0.378921\n",
      "Cost after iteration 1550: 0.376437\n",
      "Cost after iteration 1600: 0.374293\n",
      "Cost after iteration 1650: 0.372437\n",
      "Cost after iteration 1700: 0.370818\n",
      "Cost after iteration 1750: 0.369404\n",
      "Cost after iteration 1800: 0.368159\n",
      "Cost after iteration 1850: 0.367042\n",
      "Cost after iteration 1900: 0.366033\n",
      "Cost after iteration 1950: 0.365118\n",
      "Cost after iteration 2000: 0.364291\n",
      "Cost after iteration 2050: 0.363526\n",
      "Cost after iteration 2100: 0.362822\n",
      "Cost after iteration 2150: 0.362172\n",
      "Cost after iteration 2200: 0.361582\n",
      "Cost after iteration 2250: 0.361039\n",
      "Cost after iteration 2300: 0.360537\n",
      "Cost after iteration 2350: 0.360073\n",
      "Cost after iteration 2400: 0.359644\n",
      "Cost after iteration 2450: 0.359243\n",
      "Cost after iteration 2500: 0.358866\n",
      "Cost after iteration 2550: 0.358515\n",
      "Cost after iteration 2600: 0.358186\n",
      "Cost after iteration 2650: 0.357875\n",
      "Cost after iteration 2700: 0.357582\n",
      "Cost after iteration 2750: 0.357304\n",
      "Cost after iteration 2800: 0.357041\n",
      "Cost after iteration 2850: 0.356794\n",
      "Cost after iteration 2900: 0.356561\n",
      "Cost after iteration 2950: 0.356339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.736"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = { \"activations\": [\"relu\", \"sigmoid\"], \"layers_dims\" : [2, 2, 1],\n",
    "            \"learning_rate\": 0.0075, \"num_iterations\" : 3000,\n",
    "            \"print_cost\": True, \"random_seed\": random_state,v\"lambd\": 0.5}\n",
    "\n",
    "parameters        = train_model(x_train.T, y_train.T, config) \n",
    "y_test_pred       = predict(x_test.T, y_test.T, parameters, config.get(\"activations\")).T \n",
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[1.07303775, 0.14619285],\n",
       "        [0.56309786, 0.48114712]]),\n",
       " 'W2': array([[ 1.08843826, -0.74067773]]),\n",
       " 'b1': array([[ 0.10942382],\n",
       "        [-0.01702384]]),\n",
       " 'b2': array([[-0.09888404]])}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. The accuracy for S = 10 is 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.692926\n",
      "Cost after iteration 50: 0.689212\n",
      "Cost after iteration 100: 0.672043\n",
      "Cost after iteration 150: 0.615933\n",
      "Cost after iteration 200: 0.554442\n",
      "Cost after iteration 250: 0.522741\n",
      "Cost after iteration 300: 0.500189\n",
      "Cost after iteration 350: 0.475016\n",
      "Cost after iteration 400: 0.445164\n",
      "Cost after iteration 450: 0.415436\n",
      "Cost after iteration 500: 0.389899\n",
      "Cost after iteration 550: 0.366299\n",
      "Cost after iteration 600: 0.339104\n",
      "Cost after iteration 650: 0.306336\n",
      "Cost after iteration 700: 0.273213\n",
      "Cost after iteration 750: 0.245517\n",
      "Cost after iteration 800: 0.223940\n",
      "Cost after iteration 850: 0.207078\n",
      "Cost after iteration 900: 0.193555\n",
      "Cost after iteration 950: 0.182426\n",
      "Cost after iteration 1000: 0.173136\n",
      "Cost after iteration 1050: 0.165184\n",
      "Cost after iteration 1100: 0.158205\n",
      "Cost after iteration 1150: 0.151980\n",
      "Cost after iteration 1200: 0.146350\n",
      "Cost after iteration 1250: 0.141219\n",
      "Cost after iteration 1300: 0.136530\n",
      "Cost after iteration 1350: 0.132191\n",
      "Cost after iteration 1400: 0.128154\n",
      "Cost after iteration 1450: 0.124395\n",
      "Cost after iteration 1500: 0.120864\n",
      "Cost after iteration 1550: 0.117542\n",
      "Cost after iteration 1600: 0.114410\n",
      "Cost after iteration 1650: 0.111441\n",
      "Cost after iteration 1700: 0.108627\n",
      "Cost after iteration 1750: 0.105953\n",
      "Cost after iteration 1800: 0.103409\n",
      "Cost after iteration 1850: 0.100985\n",
      "Cost after iteration 1900: 0.098671\n",
      "Cost after iteration 1950: 0.096460\n",
      "Cost after iteration 2000: 0.094340\n",
      "Cost after iteration 2050: 0.092305\n",
      "Cost after iteration 2100: 0.090353\n",
      "Cost after iteration 2150: 0.088478\n",
      "Cost after iteration 2200: 0.086672\n",
      "Cost after iteration 2250: 0.084933\n",
      "Cost after iteration 2300: 0.083259\n",
      "Cost after iteration 2350: 0.081645\n",
      "Cost after iteration 2400: 0.080091\n",
      "Cost after iteration 2450: 0.078591\n",
      "Cost after iteration 2500: 0.077142\n",
      "Cost after iteration 2550: 0.075743\n",
      "Cost after iteration 2600: 0.074389\n",
      "Cost after iteration 2650: 0.073079\n",
      "Cost after iteration 2700: 0.071811\n",
      "Cost after iteration 2750: 0.070583\n",
      "Cost after iteration 2800: 0.069394\n",
      "Cost after iteration 2850: 0.068243\n",
      "Cost after iteration 2900: 0.067127\n",
      "Cost after iteration 2950: 0.066045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"activations\": [\"relu\", \"sigmoid\"],\n",
    "    \"layers_dims\" : [2, 10, 1], #  2-layer model\n",
    "    \"learning_rate\": 0.0075,\n",
    "    \"num_iterations\" : 3000,\n",
    "    \"print_cost\": True,\n",
    "    \"random_seed\": random_state,\n",
    "    \"lambd\": 0.5\n",
    "}\n",
    "\n",
    "parameters  = train_model(x_train.T, y_train.T, config)\n",
    "y_test_pred = predict(x_test.T, y_test.T, parameters, config.get(\"activations\")).T \n",
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 1.09113153,  0.17470247],\n",
       "        [ 0.34042289,  0.30449958],\n",
       "        [-0.02208754, -0.04524017],\n",
       "        [ 0.54197801,  0.48489009],\n",
       "        [ 0.11011674,  0.09852332],\n",
       "        [-0.04629777, -0.09483148],\n",
       "        [-0.22306181, -0.45705816],\n",
       "        [-0.02155755, -0.04402689],\n",
       "        [-0.02240468, -0.04588485],\n",
       "        [-0.17769971, -0.36404718]]),\n",
       " 'W2': array([[ 1.11054459, -0.45658311, -0.05027683, -0.72740834, -0.1477399 ,\n",
       "         -0.10540846, -0.50873899, -0.04572437, -0.050336  , -0.40506409]]),\n",
       " 'b1': array([[ 0.10991337],\n",
       "        [-0.01121258],\n",
       "        [ 0.00180702],\n",
       "        [-0.01938075],\n",
       "        [-0.00404729],\n",
       "        [ 0.00380767],\n",
       "        [ 0.01985845],\n",
       "        [ 0.00077604],\n",
       "        [ 0.0018217 ],\n",
       "        [ 0.01519968]]),\n",
       " 'b2': array([[1.37552988]])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
